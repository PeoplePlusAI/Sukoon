{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install langchain-core, langchain-core==0.1.42, langchain-openai==0.1.3, langchain==0.1.16, langgraph==0.0.39, langgraph==0.0.40, langgraph==0.0.41, langgraph==0.0.42, langgraph==0.0.43, langgraph==0.0.44, langgraph==0.0.45, langgraph==0.0.46, langgraph==0.0.47, langgraph==0.0.48, langgraph==0.0.49, langgraph==0.0.50, langgraph==0.0.51, langgraph==0.0.52, langgraph==0.0.53, langgraph==0.0.54, langgraph==0.0.55, langgraph==0.0.56, langgraph==0.0.57, langgraph==0.0.58, langgraph==0.0.59, langgraph==0.0.60, langgraph==0.0.61, langgraph==0.0.62, langgraph==0.0.63, langgraph==0.0.64, langgraph==0.0.65, langgraph==0.0.66, langgraph==0.0.67, langgraph==0.0.68, langgraph==0.0.69, langgraph==0.1.1, langgraph==0.1.10, langgraph==0.1.11, langgraph==0.1.12, langgraph==0.1.13, langgraph==0.1.14, langgraph==0.1.15, langgraph==0.1.16, langgraph==0.1.17, langgraph==0.1.19, langgraph==0.1.2, langgraph==0.1.3, langgraph==0.1.4, langgraph==0.1.5, langgraph==0.1.6, langgraph==0.1.7, langgraph==0.1.8, langgraph==0.1.9, langgraph==0.2.0, langgraph==0.2.1, langgraph==0.2.10, langgraph==0.2.11, langgraph==0.2.12, langgraph==0.2.13, langgraph==0.2.14, langgraph==0.2.15, langgraph==0.2.16, langgraph==0.2.17, langgraph==0.2.18, langgraph==0.2.19, langgraph==0.2.2, langgraph==0.2.20, langgraph==0.2.21, langgraph==0.2.22, langgraph==0.2.23, langgraph==0.2.24, langgraph==0.2.25, langgraph==0.2.26, langgraph==0.2.27, langgraph==0.2.28, langgraph==0.2.3, langgraph==0.2.32, langgraph==0.2.33, langgraph==0.2.34, langgraph==0.2.35, langgraph==0.2.4, langgraph==0.2.5, langgraph==0.2.6, langgraph==0.2.7, langgraph==0.2.8 and langgraph==0.2.9 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# langgraph\n",
    "# langchain-community\n",
    "# langchain-core\n",
    "# langchain-openai\n",
    "# langchain\n",
    "# langgraph-sdk\n",
    "# langgraph-checkpoint-sqlite\n",
    "# langsmith\n",
    "# langchainhub==0.1.15\n",
    "# langchain-openai==0.1.3\n",
    "# langchain==0.1.16\n",
    "# langchain-core==0.1.42\n",
    "# pygraphviz==1.12\n",
    "# llama-index\n",
    "# fastapi\n",
    "# uvicorn\n",
    "\n",
    "!pip install -qU langgraph langchain-community langchain-core langchain-openai langchain langgraph-sdk langgraph-checkpoint-sqlite langsmith langchainhub==0.1.15 langchain-openai==0.1.3 langchain==0.1.16 langchain-core==0.1.42 pygraphviz==1.12 llama-index fastapi uvicorn openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "# Viewing Queries and Events Using Logging\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# loading env variables \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPEN AI KEY TO BE PUT HERE\"\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from typing import Literal, Annotated\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import TypedDict, List\n",
    "from openai import OpenAI\n",
    "import os \n",
    "from langchain_openai import ChatOpenAI\n",
    "import yaml\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Define the state\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "BOT_USED = \"\"\n",
    "# Load prompts from YAML\n",
    "def load_prompts(file_path='prompts.yaml'):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "prompts = load_prompts()\n",
    "\n",
    "# Initialize OpenAI model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "# Define prompts for different agents\n",
    "planner_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a planner agent that decides which specialized agent to call based on the user's input. Respond with one of 'suicide_prevention', 'conversational', 'anger_management', 'motivational', or 'mindfulness' based on the user's emotion.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompts['empathetic_agent_prompt']),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "suicide_prevention_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompts['suicide_prevention_agent_prompt']),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "\n",
    "anger_management_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompts['anger_prevention_agent_prompt']),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "motivational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompts['motivational_agent_prompt']),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "mindfulness_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompts['mindfulness_agent_prompt']),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Define router\n",
    "def route_query(state: State):\n",
    "    class RouteQuery(BaseModel):\n",
    "        \"\"\"Route a user query to the most relevant node.\"\"\"\n",
    "        route: Literal[\"conversational\", \"suicide_prevention\", \"anger_management\", \"motivational\", \"mindfulness\"] = Field(\n",
    "            ...,\n",
    "            description=\"Choose the appropriate agent based on the user's emotions.\"\n",
    "        )\n",
    "    \n",
    "    structured_llm_router = model.with_structured_output(RouteQuery)\n",
    "    question_router = planner_prompt | structured_llm_router\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    resp = question_router.invoke({\"input\": last_message})\n",
    "    return resp.route\n",
    "\n",
    "# Define agent functions\n",
    "def run_conversational_agent(state: State):\n",
    "    global BOT_USED  # Declare BOT_USED as global\n",
    "    print(\"Running conversational agent\")\n",
    "    convo_model = conversational_prompt | model\n",
    "    response = convo_model.invoke(state[\"messages\"])\n",
    "    BOT_USED = \"conversational_agent\"  # Assign to global variable\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def run_suicide_prevention_agent(state: State):\n",
    "    global BOT_USED  # Declare BOT_USED as global\n",
    "    print(\"Running suicide prevention agent\")\n",
    "    concern_model = suicide_prevention_prompt | model\n",
    "    response = concern_model.invoke(state[\"messages\"])\n",
    "    BOT_USED = \"suicide_agent\"  # Assign to global variable\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def run_anger_management_agent(state: State):\n",
    "    global BOT_USED  # Declare BOT_USED as global\n",
    "    print(\"Running anger management agent\")\n",
    "    anger_model = anger_management_prompt | model\n",
    "    response = anger_model.invoke(state[\"messages\"])\n",
    "    BOT_USED = \"anger_agent\"  # Assign to global variable\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def run_motivational_agent(state: State):\n",
    "    global BOT_USED  # Declare BOT_USED as global\n",
    "    print(\"Running motivational agent\")\n",
    "    motivation_model = motivational_prompt | model\n",
    "    response = motivation_model.invoke(state[\"messages\"])\n",
    "    BOT_USED = \"motivational_agent\"  # Assign to global variable\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def run_mindfulness_agent(state: State):\n",
    "    global BOT_USED  # Declare BOT_USED as global\n",
    "    print(\"Running mindfulness agent\")\n",
    "    mindfulness_model = mindfulness_prompt | model\n",
    "    response = mindfulness_model.invoke(state[\"messages\"])\n",
    "    BOT_USED = \"mindfulness_agent\"  # Assign to global variable\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes for each agent\n",
    "workflow.add_node(\"conversational\", run_conversational_agent)\n",
    "workflow.add_node(\"suicide_prevention\", run_suicide_prevention_agent)\n",
    "workflow.add_node(\"anger_management\", run_anger_management_agent)\n",
    "workflow.add_node(\"motivational\", run_motivational_agent)\n",
    "workflow.add_node(\"mindfulness\", run_mindfulness_agent)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_query,\n",
    "    {\n",
    "        \"conversational\": \"conversational\",\n",
    "        \"suicide_prevention\": \"suicide_prevention\",\n",
    "        \"anger_management\": \"anger_management\",\n",
    "        \"motivational\": \"motivational\",\n",
    "        \"mindfulness\": \"mindfulness\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"conversational\", END)\n",
    "workflow.add_edge(\"suicide_prevention\", END)\n",
    "workflow.add_edge(\"anger_management\", END)\n",
    "workflow.add_edge(\"motivational\", END)\n",
    "workflow.add_edge(\"mindfulness\", END)\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Function to run a conversation turn\n",
    "def chat(message: str, config: dict):\n",
    "    result = graph.invoke({\"messages\": [HumanMessage(content=message)]}, config=config)\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "        \n",
    "    # Open a CSV file for writing\n",
    "    with open('chat_responses.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"User Input\", \"Triggered Agent\", \"Response\"])  # Write the header\n",
    "\n",
    "        # i = 0\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"Bot: Goodbye!\")\n",
    "                break\n",
    "                \n",
    "            response = chat(user_input, config)\n",
    "            # response = response[\"messages\"].content\n",
    "            response = response['messages'][-1].content\n",
    "            print(f\"Sukoon: {response}\")\n",
    "\n",
    "            # Log the interaction to the CSV\n",
    "            writer.writerow([user_input, BOT_USED, response])  # Write the data\n",
    "            print(BOT_USED,\"**********\")\n",
    "            BOT_USED = \"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I understand you're feeling angry right now, and it's okay to feel that way. Can you tell me what triggered your anger today? Would you like to try some deep breathing exercises to calm down? What small change could help you feel more in control of your emotions right now?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 1454, 'total_tokens': 1509, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1024}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6b68a8204b', 'finish_reason': 'stop', 'logprobs': None}, id='run-97f50719-c7e7-4b37-b2ea-3a484a066fc0-0', usage_metadata={'input_tokens': 1454, 'output_tokens': 55, 'total_tokens': 1509, 'input_token_details': {'cache_read': 1024}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
